{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF and IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOF while scanning triple-quoted string literal (<ipython-input-1-595885851bb7>, line 64)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-595885851bb7>\"\u001b[1;36m, line \u001b[1;32m64\u001b[0m\n\u001b[1;33m    \u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m EOF while scanning triple-quoted string literal\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "#reference : \"https://towardsdatascience.com/text-summarization-using-tf-idf-e64a0644ace3\"\n",
    "\n",
    "\n",
    "\n",
    "Let’s take an example and understand TFIDF.\n",
    "1)This mobile is cheap and this mobile has the best features when compared to at&t mobile. this is the best mobile in this cost.\n",
    "\n",
    "2)This mobile is bad.\n",
    "\n",
    "3)This mobile is looking good.\n",
    "\n",
    "In the bag of words, we give frequency value for a word.\n",
    "\n",
    "Let’s take the first feedback, mobile is the word occurred four times. and cheap is the word that occurred one time.\n",
    "\n",
    "In the bag of words, we give importance ie more weightage to the words that are frequently occurred in the document.\n",
    "\n",
    "Consider a situation like this.\n",
    "\n",
    "Mobile is the word that occurred in almost all the documents.\n",
    "\n",
    "Will this word helps in analyzing the feedback? no.\n",
    "\n",
    "It present in all the documents ie both positive and negative feedback.\n",
    "\n",
    "The bag of words is not taking into consideration the frequency of words that occurred in the entire document corpus.\n",
    "\n",
    "TFIDF will take into consideration.\n",
    "\n",
    "TF means term frequency.\n",
    "\n",
    "TF = No Of times word occurred in the document/Total no of words in the document.\n",
    "\n",
    "TF value is high when the word occurred more no of times in the document.\n",
    "\n",
    "IDF means Inverse document frequency.\n",
    "\n",
    "IDF = log(N/ni).\n",
    "\n",
    "N = Total no of documents\n",
    "\n",
    "ni = no of documents word i present.\n",
    "\n",
    "IDF value is high if the word occurred in less no of documents.\n",
    "\n",
    "TFIDF = TF * IDF\n",
    "\n",
    "TFIDF value will be high if the word presents more no of times in the document and less no of times in remaining documents.\n",
    "\n",
    "TFIDF is giving more weightage to rare words.\n",
    "\n",
    "Why 1 added in the denominator. because of avoiding divided by zero.\n",
    "   \n",
    "Steps fallowed to write the fallowing code.\n",
    " \n",
    " step1 : Tokenize the sentences.\n",
    " step2 : Create the Frequency matrix of the words in each sentence.\n",
    " step3 : Calculate TermFrequency and generate a matrix.\n",
    " step4 : Creating a table for documents per words.\n",
    " step5 : Calculate IDF and generate a matrix.\n",
    " step6 : 6. Calculate TF-IDF and generate a matrix.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Beans.': {'beans': 1, '.': 1}, 'I was trying to expl': {'trying': 1, 'explain': 1, 'somebody': 1, 'flying': 1, ',': 1, '’': 1, 'corn': 1, '.': 1}, 'That’s beans.': {'’': 1, 'beans': 1, '.': 1}, 'And they were very i': {'impressed': 1, 'agricultural': 1, 'knowledge': 1, '.': 1}, 'Please give it up fo': {'please': 1, 'give': 1, 'amaury': 1, 'outstanding': 1, 'introduction': 1, '.': 1}, 'I have a bunch of go': {'bunch': 1, 'good': 1, 'friends': 1, 'today': 1, ',': 5, 'including': 1, 'somebody': 1, 'served': 1, 'one': 1, 'finest': 1, 'senators': 1, 'country': 1, '’': 1, 'lucky': 1, 'senator': 1, 'dick': 1, 'durbin': 1, '.': 1}, 'I also noticed, by t': {'also': 1, 'noticed': 1, ',': 4, 'way': 1, 'former': 1, 'governor': 1, 'edgar': 1, '’': 1, 'seen': 1, 'long': 1, 'time': 1, 'somehow': 1, 'aged': 1, '.': 1}, 'And it’s great to se': {'’': 1, 'great': 1, 'see': 1, ',': 1, 'governor': 1, '.': 1}, 'I want to thank Pres': {'want': 1, 'thank': 1, 'president': 1, 'killeen': 1, 'everybody': 1, 'u': 1, 'system': 1, 'making': 1, 'possible': 1, 'today': 1, '.': 1}, 'And I am deeply hono': {'deeply': 1, 'honored': 1, 'paul': 1, 'douglas': 1, 'award': 1, 'given': 1, '.': 1}, 'He is somebody who s': {'somebody': 1, 'set': 1, 'path': 1, 'much': 1, 'outstanding': 1, 'public': 1, 'service': 1, 'illinois': 1, '.': 1}, 'Now, I want to start': {',': 1, 'want': 1, 'start': 1, 'addressing': 1, 'elephant': 1, 'room': 1, '.': 1}, 'I know people are st': {'know': 1, 'people': 1, 'still': 1, 'wondering': 1, '’': 1, 'speak': 1, 'commencement': 1, '.': 1}}\n",
      "zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz The above shown is frequency matrixzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz\n",
      "{'Beans.': {'beans': 0.5, '.': 0.5}, 'I was trying to expl': {'trying': 0.125, 'explain': 0.125, 'somebody': 0.125, 'flying': 0.125, ',': 0.125, '’': 0.125, 'corn': 0.125, '.': 0.125}, 'That’s beans.': {'’': 0.3333333333333333, 'beans': 0.3333333333333333, '.': 0.3333333333333333}, 'And they were very i': {'impressed': 0.25, 'agricultural': 0.25, 'knowledge': 0.25, '.': 0.25}, 'Please give it up fo': {'please': 0.16666666666666666, 'give': 0.16666666666666666, 'amaury': 0.16666666666666666, 'outstanding': 0.16666666666666666, 'introduction': 0.16666666666666666, '.': 0.16666666666666666}, 'I have a bunch of go': {'bunch': 0.05555555555555555, 'good': 0.05555555555555555, 'friends': 0.05555555555555555, 'today': 0.05555555555555555, ',': 0.2777777777777778, 'including': 0.05555555555555555, 'somebody': 0.05555555555555555, 'served': 0.05555555555555555, 'one': 0.05555555555555555, 'finest': 0.05555555555555555, 'senators': 0.05555555555555555, 'country': 0.05555555555555555, '’': 0.05555555555555555, 'lucky': 0.05555555555555555, 'senator': 0.05555555555555555, 'dick': 0.05555555555555555, 'durbin': 0.05555555555555555, '.': 0.05555555555555555}, 'I also noticed, by t': {'also': 0.07142857142857142, 'noticed': 0.07142857142857142, ',': 0.2857142857142857, 'way': 0.07142857142857142, 'former': 0.07142857142857142, 'governor': 0.07142857142857142, 'edgar': 0.07142857142857142, '’': 0.07142857142857142, 'seen': 0.07142857142857142, 'long': 0.07142857142857142, 'time': 0.07142857142857142, 'somehow': 0.07142857142857142, 'aged': 0.07142857142857142, '.': 0.07142857142857142}, 'And it’s great to se': {'’': 0.16666666666666666, 'great': 0.16666666666666666, 'see': 0.16666666666666666, ',': 0.16666666666666666, 'governor': 0.16666666666666666, '.': 0.16666666666666666}, 'I want to thank Pres': {'want': 0.09090909090909091, 'thank': 0.09090909090909091, 'president': 0.09090909090909091, 'killeen': 0.09090909090909091, 'everybody': 0.09090909090909091, 'u': 0.09090909090909091, 'system': 0.09090909090909091, 'making': 0.09090909090909091, 'possible': 0.09090909090909091, 'today': 0.09090909090909091, '.': 0.09090909090909091}, 'And I am deeply hono': {'deeply': 0.14285714285714285, 'honored': 0.14285714285714285, 'paul': 0.14285714285714285, 'douglas': 0.14285714285714285, 'award': 0.14285714285714285, 'given': 0.14285714285714285, '.': 0.14285714285714285}, 'He is somebody who s': {'somebody': 0.1111111111111111, 'set': 0.1111111111111111, 'path': 0.1111111111111111, 'much': 0.1111111111111111, 'outstanding': 0.1111111111111111, 'public': 0.1111111111111111, 'service': 0.1111111111111111, 'illinois': 0.1111111111111111, '.': 0.1111111111111111}, 'Now, I want to start': {',': 0.14285714285714285, 'want': 0.14285714285714285, 'start': 0.14285714285714285, 'addressing': 0.14285714285714285, 'elephant': 0.14285714285714285, 'room': 0.14285714285714285, '.': 0.14285714285714285}, 'I know people are st': {'know': 0.125, 'people': 0.125, 'still': 0.125, 'wondering': 0.125, '’': 0.125, 'speak': 0.125, 'commencement': 0.125, '.': 0.125}}\n",
      "zzzzzzzzzzzzzzzzzzzzzzzzzzz The above shon is tf matrix zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "<class 'float'>\n",
      "1.0\n",
      "{'Beans.': {'beans': 1.1139433523068367, '.': 1.1139433523068367}, 'I was trying to expl': {'trying': 1.1139433523068367, 'explain': 1.1139433523068367, 'somebody': 1.1139433523068367, 'flying': 1.1139433523068367, ',': 1.1139433523068367, '’': 1.1139433523068367, 'corn': 1.1139433523068367, '.': 1.1139433523068367}, 'That’s beans.': {'’': 1.1139433523068367, 'beans': 1.1139433523068367, '.': 1.1139433523068367}, 'And they were very i': {'impressed': 1.1139433523068367, 'agricultural': 1.1139433523068367, 'knowledge': 1.1139433523068367, '.': 1.1139433523068367}, 'Please give it up fo': {'please': 1.1139433523068367, 'give': 1.1139433523068367, 'amaury': 1.1139433523068367, 'outstanding': 1.1139433523068367, 'introduction': 1.1139433523068367, '.': 1.1139433523068367}, 'I have a bunch of go': {'bunch': 1.1139433523068367, 'good': 1.1139433523068367, 'friends': 1.1139433523068367, 'today': 1.1139433523068367, ',': 1.1139433523068367, 'including': 1.1139433523068367, 'somebody': 1.1139433523068367, 'served': 1.1139433523068367, 'one': 1.1139433523068367, 'finest': 1.1139433523068367, 'senators': 1.1139433523068367, 'country': 1.1139433523068367, '’': 1.1139433523068367, 'lucky': 1.1139433523068367, 'senator': 1.1139433523068367, 'dick': 1.1139433523068367, 'durbin': 1.1139433523068367, '.': 1.1139433523068367}, 'I also noticed, by t': {'also': 1.1139433523068367, 'noticed': 1.1139433523068367, ',': 1.1139433523068367, 'way': 1.1139433523068367, 'former': 1.1139433523068367, 'governor': 1.1139433523068367, 'edgar': 1.1139433523068367, '’': 1.1139433523068367, 'seen': 1.1139433523068367, 'long': 1.1139433523068367, 'time': 1.1139433523068367, 'somehow': 1.1139433523068367, 'aged': 1.1139433523068367, '.': 1.1139433523068367}, 'And it’s great to se': {'’': 1.1139433523068367, 'great': 1.1139433523068367, 'see': 1.1139433523068367, ',': 1.1139433523068367, 'governor': 1.1139433523068367, '.': 1.1139433523068367}, 'I want to thank Pres': {'want': 1.1139433523068367, 'thank': 1.1139433523068367, 'president': 1.1139433523068367, 'killeen': 1.1139433523068367, 'everybody': 1.1139433523068367, 'u': 1.1139433523068367, 'system': 1.1139433523068367, 'making': 1.1139433523068367, 'possible': 1.1139433523068367, 'today': 1.1139433523068367, '.': 1.1139433523068367}, 'And I am deeply hono': {'deeply': 1.1139433523068367, 'honored': 1.1139433523068367, 'paul': 1.1139433523068367, 'douglas': 1.1139433523068367, 'award': 1.1139433523068367, 'given': 1.1139433523068367, '.': 1.1139433523068367}, 'He is somebody who s': {'somebody': 1.1139433523068367, 'set': 1.1139433523068367, 'path': 1.1139433523068367, 'much': 1.1139433523068367, 'outstanding': 1.1139433523068367, 'public': 1.1139433523068367, 'service': 1.1139433523068367, 'illinois': 1.1139433523068367, '.': 1.1139433523068367}, 'Now, I want to start': {',': 1.1139433523068367, 'want': 1.1139433523068367, 'start': 1.1139433523068367, 'addressing': 1.1139433523068367, 'elephant': 1.1139433523068367, 'room': 1.1139433523068367, '.': 1.1139433523068367}, 'I know people are st': {'know': 1.1139433523068367, 'people': 1.1139433523068367, 'still': 1.1139433523068367, 'wondering': 1.1139433523068367, '’': 1.1139433523068367, 'speak': 1.1139433523068367, 'commencement': 1.1139433523068367, '.': 1.1139433523068367}}\n",
      "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx the above shon is IDF matrixxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
      "<class 'dict'>\n",
      "[['Beans.' {'beans': 0.5569716761534184, '.': 0.5569716761534184}]\n",
      " ['I was trying to expl'\n",
      "  {'trying': 0.1392429190383546, 'explain': 0.1392429190383546, 'somebody': 0.1392429190383546, 'flying': 0.1392429190383546, ',': 0.1392429190383546, '’': 0.1392429190383546, 'corn': 0.1392429190383546, '.': 0.1392429190383546}]\n",
      " ['That’s beans.'\n",
      "  {'’': 0.37131445076894554, 'beans': 0.37131445076894554, '.': 0.37131445076894554}]\n",
      " ['And they were very i'\n",
      "  {'impressed': 0.2784858380767092, 'agricultural': 0.2784858380767092, 'knowledge': 0.2784858380767092, '.': 0.2784858380767092}]\n",
      " ['Please give it up fo'\n",
      "  {'please': 0.18565722538447277, 'give': 0.18565722538447277, 'amaury': 0.18565722538447277, 'outstanding': 0.18565722538447277, 'introduction': 0.18565722538447277, '.': 0.18565722538447277}]\n",
      " ['I have a bunch of go'\n",
      "  {'bunch': 0.061885741794824256, 'good': 0.061885741794824256, 'friends': 0.061885741794824256, 'today': 0.061885741794824256, ',': 0.30942870897412134, 'including': 0.061885741794824256, 'somebody': 0.061885741794824256, 'served': 0.061885741794824256, 'one': 0.061885741794824256, 'finest': 0.061885741794824256, 'senators': 0.061885741794824256, 'country': 0.061885741794824256, '’': 0.061885741794824256, 'lucky': 0.061885741794824256, 'senator': 0.061885741794824256, 'dick': 0.061885741794824256, 'durbin': 0.061885741794824256, '.': 0.061885741794824256}]\n",
      " ['I also noticed, by t'\n",
      "  {'also': 0.07956738230763119, 'noticed': 0.07956738230763119, ',': 0.31826952923052476, 'way': 0.07956738230763119, 'former': 0.07956738230763119, 'governor': 0.07956738230763119, 'edgar': 0.07956738230763119, '’': 0.07956738230763119, 'seen': 0.07956738230763119, 'long': 0.07956738230763119, 'time': 0.07956738230763119, 'somehow': 0.07956738230763119, 'aged': 0.07956738230763119, '.': 0.07956738230763119}]\n",
      " ['And it’s great to se'\n",
      "  {'’': 0.18565722538447277, 'great': 0.18565722538447277, 'see': 0.18565722538447277, ',': 0.18565722538447277, 'governor': 0.18565722538447277, '.': 0.18565722538447277}]\n",
      " ['I want to thank Pres'\n",
      "  {'want': 0.10126757748243971, 'thank': 0.10126757748243971, 'president': 0.10126757748243971, 'killeen': 0.10126757748243971, 'everybody': 0.10126757748243971, 'u': 0.10126757748243971, 'system': 0.10126757748243971, 'making': 0.10126757748243971, 'possible': 0.10126757748243971, 'today': 0.10126757748243971, '.': 0.10126757748243971}]\n",
      " ['And I am deeply hono'\n",
      "  {'deeply': 0.15913476461526238, 'honored': 0.15913476461526238, 'paul': 0.15913476461526238, 'douglas': 0.15913476461526238, 'award': 0.15913476461526238, 'given': 0.15913476461526238, '.': 0.15913476461526238}]\n",
      " ['He is somebody who s'\n",
      "  {'somebody': 0.12377148358964851, 'set': 0.12377148358964851, 'path': 0.12377148358964851, 'much': 0.12377148358964851, 'outstanding': 0.12377148358964851, 'public': 0.12377148358964851, 'service': 0.12377148358964851, 'illinois': 0.12377148358964851, '.': 0.12377148358964851}]\n",
      " ['Now, I want to start'\n",
      "  {',': 0.15913476461526238, 'want': 0.15913476461526238, 'start': 0.15913476461526238, 'addressing': 0.15913476461526238, 'elephant': 0.15913476461526238, 'room': 0.15913476461526238, '.': 0.15913476461526238}]\n",
      " ['I know people are st'\n",
      "  {'know': 0.1392429190383546, 'people': 0.1392429190383546, 'still': 0.1392429190383546, 'wondering': 0.1392429190383546, '’': 0.1392429190383546, 'speak': 0.1392429190383546, 'commencement': 0.1392429190383546, '.': 0.1392429190383546}]]\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "import re \n",
    "import numpy as np \n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "\n",
    "# execute the text here as : \n",
    "text = \"\"\"Beans. I was trying to explain to somebody as we were flying in, that’s corn. \n",
    "That’s beans. And they were very impressed at my agricultural knowledge. \n",
    "Please give it up for Amaury once again for that outstanding introduction. \n",
    "I have a bunch of good friends here today, including somebody who I served with, \n",
    "who is one of the finest senators in the country, and we’re lucky to have him, your Senator, \n",
    "Dick Durbin is here. I also noticed, by the way, former Governor Edgar here, who I haven’t seen in a long time, \n",
    "and somehow he has not aged and I have. And it’s great to see you, Governor. I want to thank President Killeen \n",
    "and everybody at the U of I System for making it possible for me to be here today. \n",
    "And I am deeply honored at the Paul Douglas Award that is being given to me. \n",
    "He is somebody who set the path for so much outstanding public service here in Illinois.\n",
    "Now, I want to start by addressing the elephant in the room. I know people are still \n",
    "wondering why I didn’t speak at the commencement.\"\"\"\n",
    "\n",
    "\n",
    "# Tokenize the sentences : \n",
    "sentences = nltk.sent_tokenize(text) # NLTK function\n",
    "total_documents = len(sentences)\n",
    "\n",
    "# Create the Frequency matrix of the words in each sentence.\n",
    "# here  We calculate the frequency of words in each sentence.\n",
    "\n",
    "frequency_matrix = {}\n",
    "stopWords = set(stopwords.words(\"english\"))\n",
    "for sent in sentences:\n",
    "    freq_table = {}\n",
    "    words = nltk.word_tokenize(sent)\n",
    "    for word in words:\n",
    "       \n",
    "        word = word.lower()\n",
    "       \n",
    "        if word in stopWords:\n",
    "            \n",
    "            continue\n",
    "        \n",
    "        if word in freq_table:\n",
    "                freq_table[word] += 1\n",
    "        else:\n",
    "                freq_table[word] = 1\n",
    "\n",
    "    frequency_matrix [sent[:20]]= freq_table\n",
    "print(frequency_matrix)\n",
    "\n",
    "\n",
    "print(\"zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz The above shown is frequency matrixzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz\")\n",
    "\n",
    "# Calculating TermFrequency and generating a matrix\n",
    "tf_matrix = {}\n",
    "\n",
    "for sent, freq_table in frequency_matrix.items():\n",
    "    tf_table = {}\n",
    "   \n",
    "    count_words_in_sentence = len(freq_table)\n",
    "    for word, count in freq_table.items():\n",
    "      \n",
    "        tf_table[word] = count / count_words_in_sentence\n",
    "    \n",
    "    tf_matrix[sent] = tf_table \n",
    "print(tf_matrix)\n",
    "print(\"zzzzzzzzzzzzzzzzzzzzzzzzzzz The above shon is tf matrix zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz\")\n",
    "\n",
    "    \n",
    "    # Creating a table for documents per words\n",
    "word_per_doc_table = {}\n",
    "\n",
    "for sent, freq_table in frequency_matrix.items():\n",
    "    for word, count in freq_table.items():\n",
    "        if word in word_per_doc_table:\n",
    "            word_per_doc_table[word] += 1\n",
    "        else:\n",
    "            word_per_doc_table[word] = 1\n",
    "\n",
    "#converting the word_per_doc_table to float             \n",
    "word_per_doc_table = [float(x) for x in list(word_per_doc_table.values())]\n",
    "for word_per_doc_table in word_per_doc_table:\n",
    "    print(type(word_per_doc_table))  \n",
    "print(word_per_doc_table)  \n",
    "\n",
    " #Calculating IDF and generating a matrix\n",
    "    \n",
    "idf_matrix = {}\n",
    "\n",
    "for sent, freq_table in frequency_matrix.items():\n",
    "    idf_table = {}\n",
    "\n",
    "    for word in freq_table.keys():\n",
    "        idf_table[word] = math.log10(total_documents / float(word_per_doc_table))\n",
    "\n",
    "    idf_matrix[sent] = idf_table\n",
    "print(idf_matrix)\n",
    "print(\"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx the above shon is IDF matrixxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\")\n",
    "\n",
    " #Calculating TF-IDF and generate a matrix\n",
    "\n",
    "tf_idf_matrix = {}\n",
    "\n",
    "for (sent1, freq_table1), (sent2, freq_table2) in zip(tf_matrix.items(), idf_matrix.items()):\n",
    "\n",
    "    tf_idf_table = {}\n",
    "\n",
    "    for (word1, value1), (word2, value2) in zip(freq_table1.items(),freq_table2.items()):  \n",
    "        tf_idf_table[word1] = float(value1 * value2)\n",
    "\n",
    "    tf_idf_matrix[sent1] = tf_idf_table\n",
    "\n",
    "print(type(tf_idf_matrix))\n",
    "tf_idf_matrix = list(tf_idf_matrix.items())\n",
    "tf_idf_matrix=np.array(tf_idf_matrix)\n",
    "print(tf_idf_matrix)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
