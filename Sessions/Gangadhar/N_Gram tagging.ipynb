{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''reference :https://www.geeksforgeeks.org/confusion-matrix-machine-learning/\n",
    "              https://www.geeksforgeeks.org/python-nltk-nltk-tokenize-conditionalfreqdist/\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Unigram Models\n",
    "\n",
    "One of its characteristics is that it doesn’t take the ordering of the words into account, so the order doesn't\n",
    "make a difference in how words are tagged or split up. If you run the following code in Python, \n",
    "you’ll train a word tagging model with a Unigram Tagger.\n",
    "\n",
    "result ill be lke:\n",
    "\n",
    "[('Various', 'JJ'), ('of', 'IN'), ('the', 'AT'), ('apartments', 'NNS'), ('are', 'BER'), ('of', 'IN'), ('the', 'AT'),\n",
    " ('terrace', 'NN'), ('type', 'NN'), (',', ','), ('being', 'BEG'), ('on', 'IN'), ('the', 'AT'), ('ground', 'NN'), \n",
    " ('floor', 'NN'), ('so', 'QL'), ('that', 'CS'), ('entrance', 'NN'), ('is', 'BEZ'), ('direct', 'JJ'), ('.', '.')]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Bigram Models\n",
    "\n",
    "Bigram Models, on the other hand do care about the order of the words, so it considers the context of each word by analyzing it\n",
    "by pairs. Whereas a unigram model will tag a word independent of the other words, \n",
    "a bigram model will tag like follows (with the example, “the cat in the hat”)\n",
    "\n",
    "(the, cat)\n",
    "\n",
    "(cat, in)\n",
    "\n",
    "(in, the)\n",
    "\n",
    "(the, hat)\n",
    "\n",
    "So if you try to tag a sentence with a unigram, as done below in Python\n",
    "\n",
    "… you’ll get a slightly different result:\n",
    "\n",
    "[('Various', 'JJ'), ('of', 'IN'), ('the', 'AT'), ('apartments', 'NNS'), \n",
    " ('are', 'BER'), ('of', 'IN'), ('the', 'AT'), ('terrace', 'NN'), ('type', 'NN'), \n",
    " (',', ','), ('being', 'BEG'), ('on', 'IN'), ('the', 'AT'), ('ground', 'NN'), ('floor', 'NN'),\n",
    " ('so', 'CS'), ('that', 'CS'), ('entrance', 'NN'), ('is', 'BEZ'), ('direct', 'JJ'), ('.', '.')]\n",
    "\n",
    "Why is this important? . In many instances the order of the words might not matter at all. \n",
    "But take a sentence like “This boat is going to sink!”\n",
    "\n",
    "The word `sink` can either be a verb or noun, depending on its context. In order to figure out which one it is,\n",
    "you’d have to check the words before or after. In the case above, the word `to` let’s us know that it is, in fact, a verb. \n",
    "In a unigram model, that context would not be considered and it might be tagged incorrectly.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Various', 'JJ'), ('of', 'IN'), ('the', 'AT'), ('apartments', 'NNS'), ('are', 'BER'), ('of', 'IN'), ('the', 'AT'), ('terrace', 'NN'), ('type', 'NN'), (',', ','), ('being', 'BEG'), ('on', 'IN'), ('the', 'AT'), ('ground', 'NN'), ('floor', 'NN'), ('so', 'QL'), ('that', 'CS'), ('entrance', 'NN'), ('is', 'BEZ'), ('direct', 'JJ'), ('.', '.')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9349006503968017"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown \n",
    "brown_tagged_sents = brown.tagged_sents(categories='news')\n",
    "brown_sents = brown.sents(categories='news') \n",
    "unigram_tagger = nltk.UnigramTagger(brown_tagged_sents)\n",
    "unigram_tagger.tag(brown_sents[2007])\n",
    "print(unigram_tagger.tag(brown_sents[2007]))\n",
    "unigram_tagger.evaluate(brown_tagged_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Various', 'JJ'), ('of', 'IN'), ('the', 'AT'), ('apartments', 'NNS'), ('are', 'BER'), ('of', 'IN'), ('the', 'AT'), ('terrace', 'NN'), ('type', 'NN'), (',', ','), ('being', 'BEG'), ('on', 'IN'), ('the', 'AT'), ('ground', 'NN'), ('floor', 'NN'), ('so', 'CS'), ('that', 'CS'), ('entrance', 'NN'), ('is', 'BEZ'), ('direct', 'JJ'), ('.', '.')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7892972929967977"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown \n",
    "brown_tagged_sents = brown.tagged_sents(categories='news')\n",
    "brown_sents = brown.sents(categories='news') \n",
    "bigram_tagger = nltk.BigramTagger(brown_tagged_sents) \n",
    "bigram_tagger.tag(brown_sents[2007])\n",
    "print(bigram_tagger.tag(brown_sents[2007]))\n",
    "bigram_tagger.evaluate(brown_tagged_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.'], ['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.'], ...]\n",
      "zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz\n",
      "[[('The', 'AT'), ('Fulton', 'NP-TL'), ('County', 'NN-TL'), ('Grand', 'JJ-TL'), ('Jury', 'NN-TL'), ('said', 'VBD'), ('Friday', 'NR'), ('an', 'AT'), ('investigation', 'NN'), ('of', 'IN'), (\"Atlanta's\", 'NP$'), ('recent', 'JJ'), ('primary', 'NN'), ('election', 'NN'), ('produced', 'VBD'), ('``', '``'), ('no', 'AT'), ('evidence', 'NN'), (\"''\", \"''\"), ('that', 'CS'), ('any', 'DTI'), ('irregularities', 'NNS'), ('took', 'VBD'), ('place', 'NN'), ('.', '.')], [('The', 'AT'), ('jury', 'NN'), ('further', 'RBR'), ('said', 'VBD'), ('in', 'IN'), ('term-end', 'NN'), ('presentments', 'NNS'), ('that', 'CS'), ('the', 'AT'), ('City', 'NN-TL'), ('Executive', 'JJ-TL'), ('Committee', 'NN-TL'), (',', ','), ('which', 'WDT'), ('had', 'HVD'), ('over-all', 'JJ'), ('charge', 'NN'), ('of', 'IN'), ('the', 'AT'), ('election', 'NN'), (',', ','), ('``', '``'), ('deserves', 'VBZ'), ('the', 'AT'), ('praise', 'NN'), ('and', 'CC'), ('thanks', 'NNS'), ('of', 'IN'), ('the', 'AT'), ('City', 'NN-TL'), ('of', 'IN-TL'), ('Atlanta', 'NP-TL'), (\"''\", \"''\"), ('for', 'IN'), ('the', 'AT'), ('manner', 'NN'), ('in', 'IN'), ('which', 'WDT'), ('the', 'AT'), ('election', 'NN'), ('was', 'BEDZ'), ('conducted', 'VBN'), ('.', '.')], ...]\n",
      "<UnigramTagger: size=14394>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9349006503968017"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "brown_tagged_sents = brown.tagged_sents(categories='news')\n",
    "brown_sents = brown.sents(categories='news')\n",
    "print(brown_sents)\n",
    "print(\"zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz\")\n",
    "print(brown_tagged_sents)\n",
    "unigram_tagger = nltk.UnigramTagger(brown_tagged_sents)\n",
    "unigram_tagger.tag(brown_sents[2007])\n",
    "print(unigram_tagger)\n",
    "unigram_tagger.evaluate(brown_tagged_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4160\n",
      "4623\n",
      "[[('The', 'AT'), ('Fulton', 'NP-TL'), ('County', 'NN-TL'), ('Grand', 'JJ-TL'), ('Jury', 'NN-TL'), ('said', 'VBD'), ('Friday', 'NR'), ('an', 'AT'), ('investigation', 'NN'), ('of', 'IN'), (\"Atlanta's\", 'NP$'), ('recent', 'JJ'), ('primary', 'NN'), ('election', 'NN'), ('produced', 'VBD'), ('``', '``'), ('no', 'AT'), ('evidence', 'NN'), (\"''\", \"''\"), ('that', 'CS'), ('any', 'DTI'), ('irregularities', 'NNS'), ('took', 'VBD'), ('place', 'NN'), ('.', '.')], [('The', 'AT'), ('jury', 'NN'), ('further', 'RBR'), ('said', 'VBD'), ('in', 'IN'), ('term-end', 'NN'), ('presentments', 'NNS'), ('that', 'CS'), ('the', 'AT'), ('City', 'NN-TL'), ('Executive', 'JJ-TL'), ('Committee', 'NN-TL'), (',', ','), ('which', 'WDT'), ('had', 'HVD'), ('over-all', 'JJ'), ('charge', 'NN'), ('of', 'IN'), ('the', 'AT'), ('election', 'NN'), (',', ','), ('``', '``'), ('deserves', 'VBZ'), ('the', 'AT'), ('praise', 'NN'), ('and', 'CC'), ('thanks', 'NNS'), ('of', 'IN'), ('the', 'AT'), ('City', 'NN-TL'), ('of', 'IN-TL'), ('Atlanta', 'NP-TL'), (\"''\", \"''\"), ('for', 'IN'), ('the', 'AT'), ('manner', 'NN'), ('in', 'IN'), ('which', 'WDT'), ('the', 'AT'), ('election', 'NN'), ('was', 'BEDZ'), ('conducted', 'VBN'), ('.', '.')], ...]\n",
      "zzzzzzzzzzzzzzzzzzzzz\n",
      "[[('But', 'CC'), ('in', 'IN'), ('all', 'ABN'), ('its', 'PP$'), ('175', 'CD'), ('years', 'NNS'), (',', ','), ('not', '*'), ('a', 'AT'), ('single', 'AP'), ('Negro', 'NP'), ('student', 'NN'), ('has', 'HVZ'), ('entered', 'VBN'), ('its', 'PP$'), ('classrooms', 'NNS'), ('.', '.')], [('Last', 'AP'), ('week', 'NN'), ('Federal', 'JJ-TL'), ('District', 'NN-TL'), ('Judge', 'NN-TL'), ('William', 'NP'), ('A.', 'NP'), ('Bootle', 'NP'), ('ordered', 'VBD'), ('the', 'AT'), ('university', 'NN'), ('to', 'TO'), ('admit', 'VB'), ('immediately', 'RB'), ('a', 'AT'), ('``', '``'), ('qualified', 'VBN'), (\"''\", \"''\"), ('Negro', 'NP'), ('boy', 'NN'), ('and', 'CC'), ('girl', 'NN'), ('.', '.')], ...]\n",
      "0.8121200039868434\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9353630649241612"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#splitting the data, training on 90% and testing on the remaining 10%:\n",
    "\n",
    "size = int(len(brown_tagged_sents) * 0.9)\n",
    "print(size)\n",
    "print(len(brown_tagged_sents))\n",
    "\n",
    "#trained data =90% of the data\n",
    "train_sents = brown_tagged_sents[:size]\n",
    "print(train_sents)\n",
    "print(\"zzzzzzzzzzzzzzzzzzzzz\")\n",
    "\n",
    "#test data =10% of the data that is nothing but the remaining data after trained data.\n",
    "test_sents = brown_tagged_sents[size:]\n",
    "print(test_sents)\n",
    "unigram_tagger = nltk.UnigramTagger(train_sents)\n",
    "unigram_tagger.evaluate(test_sents)\n",
    "print(unigram_tagger.evaluate(test_sents))\n",
    "unigram_tagger.evaluate(train_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8452108043456593"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "#Combining Taggers\n",
    "#Combining taggers with backoff tagging\n",
    "'''Backoff tagging is one of the core features of SequentialBackoffTagger. \n",
    "It allows you to chain taggers together so that if one tagger doesn't know how to tag a word, \n",
    "it can pass the word on to the next backoff tagger. If that one can't do it, it can pass the word on to the next backoff tagger,\n",
    "and so on until there are no backoff taggers left to check.'''\n",
    "\n",
    "t0 = nltk.DefaultTagger('NN')\n",
    "t1 = nltk.UnigramTagger(train_sents, backoff=t0)\n",
    "t2 = nltk.BigramTagger(train_sents, backoff=t1)\n",
    "print(t0.evaluate(test_sents))\n",
    "print(t1.evaluate(test_sents))\n",
    "t2.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.049297702068029296"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cases of part-of-speech ambiguity \n",
    "cfd = nltk.ConditionalFreqDist(\n",
    "            ((x[1], y[1], z[0]), z[1])\n",
    "            for sent in brown_tagged_sents\n",
    "            for x, y, z in nltk.trigrams(sent))\n",
    "ambiguous_contexts = [c for c in cfd.conditions() if len(cfd[c]) > 1]\n",
    "sum(cfd[c].N() for c in ambiguous_contexts) / cfd.N()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tagged data from treebank... \n",
      "Read testing data (200 sents/5251 wds)\n",
      "Read training data (800 sents/19933 wds)\n",
      "Read baseline data (800 sents/19933 wds) [reused the training set]\n",
      "Trained baseline tagger\n",
      "    Accuracy on test set: 0.8366\n",
      "Training tbl tagger...\n",
      "TBL train (fast) (seqs: 800; tokens: 19933; tpls: 24; min score: 3; min acc: None)\n",
      "Finding initial useful rules...\n",
      "    Found 12799 useful rules.\n",
      "\n",
      "           B      |\n",
      "   S   F   r   O  |        Score = Fixed - Broken\n",
      "   c   i   o   t  |  R     Fixed = num tags changed incorrect -> correct\n",
      "   o   x   k   h  |  u     Broken = num tags changed correct -> incorrect\n",
      "   r   e   e   e  |  l     Other = num tags changed incorrect -> incorrect\n",
      "   e   d   n   r  |  e\n",
      "------------------+-------------------------------------------------------\n",
      "  23  23   0   0  | POS->VBZ if Pos:PRP@[-2,-1]\n",
      "  18  19   1   0  | NN->VB if Pos:-NONE-@[-2] & Pos:TO@[-1]\n",
      "  14  14   0   0  | VBP->VB if Pos:MD@[-2,-1]\n",
      "  12  12   0   0  | VBP->VB if Pos:TO@[-1]\n",
      "  11  11   0   0  | VBD->VBN if Pos:VBD@[-1]\n",
      "  11  11   0   0  | IN->WDT if Pos:-NONE-@[1] & Pos:VBP@[2]\n",
      "  10  11   1   0  | VBN->VBD if Pos:PRP@[-1]\n",
      "   9  10   1   0  | VBD->VBN if Pos:VBZ@[-1]\n",
      "   8   8   0   0  | NN->VB if Pos:MD@[-1]\n",
      "   7   7   0   1  | VB->NN if Pos:DT@[-1]\n",
      "   7   7   0   0  | VB->VBP if Pos:PRP@[-1]\n",
      "   7   7   0   0  | IN->WDT if Pos:-NONE-@[1] & Pos:VBZ@[2]\n",
      "   7   8   1   0  | IN->RB if Word:as@[2]\n",
      "   6   6   0   0  | VBD->VBN if Pos:VBP@[-2,-1]\n",
      "   6   6   0   1  | IN->WDT if Pos:-NONE-@[1] & Pos:VBD@[2]\n",
      "   5   5   0   0  | POS->VBZ if Pos:-NONE-@[-1]\n",
      "   5   5   0   0  | VB->VBP if Pos:NNS@[-1]\n",
      "   5   5   0   0  | VBD->VBN if Word:be@[-2,-1]\n",
      "   4   4   0   0  | POS->VBZ if Pos:``@[-2]\n",
      "   4   4   0   0  | VBP->VB if Pos:VBD@[-2,-1]\n",
      "   4   6   2   3  | RP->RB if Pos:CD@[1,2]\n",
      "   4   4   0   0  | RB->JJ if Pos:DT@[-1] & Pos:NN@[1]\n",
      "   4   4   0   0  | NN->VBP if Pos:NNS@[-2] & Pos:RB@[-1]\n",
      "   4   5   1   0  | VBN->VBD if Pos:NNP@[-2] & Pos:NNP@[-1]\n",
      "   4   4   0   0  | IN->WDT if Pos:-NONE-@[1] & Pos:MD@[2]\n",
      "   4   8   4   0  | VBD->VBN if Word:*@[1]\n",
      "   4   4   0   0  | JJS->RBS if Word:most@[0] & Word:the@[-1] & Pos:DT@[-1]\n",
      "   3   3   0   0  | VBD->VBN if Pos:VBN@[-1]\n",
      "   3   4   1   0  | VBN->VB if Pos:TO@[-1]\n",
      "   3   4   1   1  | IN->RB if Pos:.@[1]\n",
      "   3   3   0   0  | JJ->RB if Pos:VBD@[1]\n",
      "   3   3   0   0  | PRP$->PRP if Pos:TO@[1]\n",
      "   3   3   0   0  | NN->VBP if Pos:NNS@[-1] & Pos:DT@[1]\n",
      "   3   3   0   0  | VBP->VB if Word:n't@[-2,-1]\n",
      "Trained tbl tagger in 7.83 seconds\n",
      "    Accuracy on test set: 0.8572\n",
      "Tagging the test data\n"
     ]
    }
   ],
   "source": [
    "#transformation based tagger\n",
    "import nltk\n",
    "from nltk.tbl import demo as issue1829\n",
    "issue1829.demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4 2]\n",
      " [1 3]]\n",
      "Accuracy Score : 0.7\n",
      "Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.67      0.73         6\n",
      "           1       0.60      0.75      0.67         4\n",
      "\n",
      "    accuracy                           0.70        10\n",
      "   macro avg       0.70      0.71      0.70        10\n",
      "weighted avg       0.72      0.70      0.70        10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Python script for confusion matrix creation. \n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import classification_report \n",
    "  \n",
    "actual = [1, 1, 0, 1, 0, 0, 1, 0, 0, 0] \n",
    "predicted = [1, 0, 0, 1, 0, 0, 1, 1, 1, 0] \n",
    "results = confusion_matrix(actual, predicted) \n",
    "print(results)\n",
    "print ('Accuracy Score :',accuracy_score(actual, predicted))\n",
    "print( 'Report : ')\n",
    "print( classification_report(actual, predicted) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "formulae for acuuracy is accuracy = (TP +TN)/(TP+TN+FP+FN)\n",
    "f1score=(precission*recall)/(precision + recall)\n",
    "precission=TP/(TP+FP)\n",
    "Recall=TP/(TP+FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
